{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Train the various algorithms and show your results. \n",
    "\n",
    "You must plot the reward obtained by your agent per step and the total regret accumulated so far.\n",
    "\n",
    "This one is an open ended assignment, so feel free to play around. Extra credit for more beautiful plots (you can check out Seaborn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m\n\u001b[0;32m     32\u001b[0m bandit \u001b[38;5;241m=\u001b[39m Bandit(num_arms, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBernoulli\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     GreedyAgent(bandit, initialQ\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     36\u001b[0m     epsGreedyAgent(bandit, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     ThompsonSamplerAgent(bandit)\n\u001b[0;32m     40\u001b[0m ]\n\u001b[1;32m---> 42\u001b[0m regrets \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbandit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m plot_regret(regrets, num_steps)\n",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m, in \u001b[0;36mtrain_agents\u001b[1;34m(bandit, agents, num_steps)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[1;32m---> 13\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         regrets[agent]\u001b[38;5;241m.\u001b[39mappend(agent\u001b[38;5;241m.\u001b[39mbandit\u001b[38;5;241m.\u001b[39mget_regret())\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m regrets\n",
      "File \u001b[1;32mc:\\Users\\anshu\\Desktop\\HANDS_ON_RL_SoC\\HandsOnRL-SOC\\week3\\Assignment\\agents.py:24\u001b[0m, in \u001b[0;36mAgent.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     23\u001b[0m     choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction()\n\u001b[1;32m---> 24\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbandit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumiters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\anshu\\Desktop\\HANDS_ON_RL_SoC\\HandsOnRL-SOC\\week3\\Assignment\\bandits.py:35\u001b[0m, in \u001b[0;36mBandit.choose\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose\u001b[39m(\u001b[38;5;28mself\u001b[39m,k:\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# returns 1 with probability = self.probs[k]\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBernoulli\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bandits import Bandit\n",
    "from agents import *\n",
    "\n",
    "def train_agents(bandit, agents, num_steps):\n",
    "    regrets = {agent: [] for agent in agents}\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        for agent in agents:\n",
    "            agent.act()\n",
    "            regrets[agent].append(agent.bandit.get_regret())\n",
    "    \n",
    "    return regrets\n",
    "\n",
    "def plot_regret(regrets, num_steps):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for agent, regret in regrets.items():\n",
    "        plt.plot(range(num_steps), regret, label=type(agent).__name__)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Cumulative Regret')\n",
    "    plt.title('Cumulative Regret of Different Bandit Algorithms')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_arms = 10\n",
    "    num_steps = 1000\n",
    "\n",
    "    bandit = Bandit(num_arms, \"Bernoulli\")\n",
    "\n",
    "    agents = [\n",
    "        GreedyAgent(bandit, initialQ=0),\n",
    "        epsGreedyAgent(bandit, epsilon=0.1),\n",
    "        UCBAAgent(bandit, c=2),\n",
    "        GradientBanditAgent(bandit, alpha=0.1),\n",
    "        ThompsonSamplerAgent(bandit)\n",
    "    ]\n",
    "\n",
    "    regrets = train_agents(bandit, agents, num_steps)\n",
    "    plot_regret(regrets, num_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
